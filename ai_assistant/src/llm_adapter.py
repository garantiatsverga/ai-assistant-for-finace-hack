from typing import List, Optional, AsyncGenerator
import aiohttp
import json
import logging
import re
import asyncio
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class LLMError(Exception):
    """–û—à–∏–±–∫–∏ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å LLM"""
    pass

class LLMAdapter:
    """–ê–¥–∞–ø—Ç–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é —á–µ—Ä–µ–∑ Ollama API"""
    
    def __init__(self, model_name: str = "qwen2.5:0.5b", base_url: str = "http://localhost:11434", timeout: int = 30):
        self.model_name = model_name
        self.base_url = base_url
        self.timeout = timeout
        
        # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞/SQL
        self._code_patterns = re.compile(
            r'\b(sql|select|insert|update|delete|drop|create table|execute|eval|exec)\b|–Ω–∞–ø–∏—Å–∞—Ç—å –∫–æ–¥|—Å–≥–µ–Ω–µ—Ä–∏—Ä—É–π sql',
            flags=re.IGNORECASE
        )

    def _is_code_request(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–ø—Ä–æ—Å –∑–∞–ø—Ä–æ—Å–æ–º –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∫–æ–¥–∞"""
        if not text:
            return False
        return bool(self._code_patterns.search(text))

    @asynccontextmanager
    async def _create_session(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ aiohttp —Å–µ—Å—Å–∏–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º"""
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        session = aiohttp.ClientSession(timeout=timeout)
        try:
            yield session
        finally:
            await session.close()

    async def generate_answer_streaming(self, 
                                    question: str, 
                                    context_docs: List[str],
                                    deep_think: bool = False,
                                    flags: List[str] = None) -> AsyncGenerator[str, None]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å–æ —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–ª–∞–≥–æ–≤"""
        
        if flags is None:
            flags = []
        
        logger.info(f"üîß [LLM-1] –ù–∞—á–∞–ª–æ generate_answer_streaming. –§–ª–∞–≥–∏: {flags}")
        
        # –ó–∞—â–∏—Ç–Ω—ã–π —Å–ª–æ–π: –æ—Ç–∫–∞–∑—ã–≤–∞–µ–º –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ –∫–æ–¥–∞/SQL (–µ—Å–ª–∏ –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ —Ñ–ª–∞–≥–æ–º)
        if self._is_code_request(question) and '-nocode' not in flags:
            logger.info("üîß [LLM-1a] –ó–∞–ø—Ä–æ—Å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω (–∫–æ–¥/SQL)")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –º–æ–≥—É –ø–æ–º–æ–≥–∞—Ç—å —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π –∏—Å–ø–æ–ª–Ω—è–µ–º–æ–≥–æ –∫–æ–¥–∞ –∏–ª–∏ SQL-–∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."
            return

        logger.info("üîß [LLM-2] –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç")
        prompt = self._create_prompt(question, context_docs, deep_think, flags)
        
        logger.info("üîß [LLM-3] –ù–∞—á–∏–Ω–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç Ollama")
        try:
            chunk_count = 0
            async for chunk in self._stream_from_ollama(prompt):
                logger.info(f"üîß [LLM-3a] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º chunk #{chunk_count}: '{chunk}'")
                chunk_count += 1
                
                # –ï—Å–ª–∏ –∞–∫—Ç–∏–≤–µ–Ω –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º, —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏
                if '-simple' in flags:
                    chunk = self._simplify_response(chunk)
                    
                yield chunk
                    
            logger.info(f"üîß [LLM-4] generate_answer_streaming –∑–∞–≤–µ—Ä—à–µ–Ω. –ß–∞–Ω–∫–æ–≤: {chunk_count}")
            
        except Exception as e:
            logger.error(f"üîß [LLM-ERROR] –û—à–∏–±–∫–∞ –≤ generate_answer_streaming: {e}")
            yield self._fallback_answer(context_docs)

    async def _stream_from_ollama(self, prompt: str) -> AsyncGenerator[str, None]:
        """–ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø–æ—Ç–æ–∫–æ–≤–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama API"""
        url = f"{self.base_url}/api/generate"
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.1,
                "top_p": 0.9,
                "num_predict": 500,
                "repeat_penalty": 1.2
            }
        }
        
        logger.info(f"üîß –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ Ollama...")
        
        async with self._create_session() as session:
            try:
                async with session.post(url, json=payload) as response:
                    logger.info(f"üîß –°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞: {response.status}")
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Ollama API: {response.status} - {error_text}")
                        yield "‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –º–æ–¥–µ–ª—å—é."
                        return
                    
                    logger.info("üîß –ù–∞—á–∏–Ω–∞–µ–º —á—Ç–µ–Ω–∏–µ –ø–æ—Ç–æ–∫–∞...")
                    
                    # –ß–∏—Ç–∞–µ–º –ø–æ—Ç–æ–∫ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
                    async for line_bytes in response.content:
                        if line_bytes:
                            line = line_bytes.decode('utf-8').strip()
                            
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                            if not line:
                                continue
                                
                            try:
                                data = json.loads(line)
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
                                if data.get('done', False):
                                    logger.info("üîß –°—Ç—Ä–∏–º–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω")
                                    break
                                
                                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º response –µ—Å–ª–∏ –µ—Å—Ç—å
                                if 'response' in data and data['response']:
                                    yield data['response']
                                    
                            except json.JSONDecodeError:
                                logger.warning(f"üîß –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSON: {line}")
                                continue
                            except Exception as e:
                                logger.warning(f"üîß –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                                continue
                    
                    logger.info("üîß –ü–æ—Ç–æ–∫ –∑–∞–≤–µ—Ä—à–µ–Ω")
                                    
            except asyncio.TimeoutError:
                logger.error("‚è∞ –¢–∞–π–º–∞—É—Ç –ø—Ä–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–µ")
                yield "‚è∞ –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞."
            except aiohttp.ClientConnectorError:
                logger.error("üîå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama")
                yield "üîå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏."
            except Exception as e:
                logger.error(f"üí• –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                yield "‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."

    async def generate_answer(self, 
                            question: str, 
                            context_docs: List[str],
                            deep_think: bool = False) -> str:
        """–û–±—ã—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        full_response = ""
        async for chunk in self.generate_answer_streaming(question, context_docs, deep_think):
            full_response += chunk
        return full_response

    def _create_prompt(self, 
                    question: str, 
                    context_docs: List[str],
                    deep_think: bool,
                    flags: List[str]) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å —É—á–µ—Ç–æ–º —Ñ–ª–∞–≥–æ–≤"""
        
        context_text = "\n".join(context_docs) if context_docs else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
        
        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        base_prompt = f"""–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å: {question}

    –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
    {context_text}

    –û—Ç–≤–µ—Ç:"""
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–ª–∞–≥–æ–≤
        if '-simple' in flags:
            base_prompt = f"""–í–æ–ø—Ä–æ—Å: {question}
    –î–∞–Ω–Ω—ã–µ: {context_text}
    –ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç:"""
        
        return base_prompt
    
    def _fallback_answer(self, context_docs: List[str]) -> str:
        """–û—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ LLM"""
        if not context_docs:
            return "‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        return "–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n" + "\n".join(
            [f"‚Ä¢ {doc[:100]}..." if len(doc) > 100 else f"‚Ä¢ {doc}" 
             for doc in context_docs[:3]]
        )

    def _simplify_response(self, text: str) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        # –£–±–∏—Ä–∞–µ–º —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è –∏ –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
        simplifications = {
            "–ö–æ–Ω–µ—á–Ω–æ,": "",
            "–†–∞–¥ –ø–æ–º–æ—á—å!": "",
            "–í–æ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å:": "",
            "–°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏,": ""
        }
        
        for old, new in simplifications.items():
            text = text.replace(old, new)
        
        return text.strip()