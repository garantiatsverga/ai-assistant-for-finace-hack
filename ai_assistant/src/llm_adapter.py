from typing import List, Optional, AsyncGenerator
import aiohttp
import json
import logging
import re
import asyncio
from contextlib import asynccontextmanager

logger = logging.getLogger(__name__)

class LLMError(Exception):
    """ÐžÑˆÐ¸Ð±ÐºÐ¸ Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ LLM"""
    pass

class LLMAdapter:
    """ÐÐ´Ð°Ð¿Ñ‚ÐµÑ€ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ Ñ‡ÐµÑ€ÐµÐ· Ollama API"""
    
    def __init__(self, model_name: str = "qwen2.5:0.5b", base_url: str = "http://localhost:11434", timeout: int = 30):
        self.model_name = model_name
        self.base_url = base_url
        self.timeout = timeout
        
        # Ð¨Ð°Ð±Ð»Ð¾Ð½Ñ‹ Ð´Ð»Ñ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð½Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ ÐºÐ¾Ð´Ð°/SQL
        self._code_patterns = re.compile(
            r'\b(sql|select|insert|update|delete|drop|create table|execute|eval|exec)\b|Ð½Ð°Ð¿Ð¸ÑÐ°Ñ‚ÑŒ ÐºÐ¾Ð´|ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ sql',
            flags=re.IGNORECASE
        )

    def _is_code_request(self, text: str) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ°, ÑÐ²Ð»ÑÐµÑ‚ÑÑ Ð»Ð¸ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð¼ Ð½Ð° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÑŽ ÐºÐ¾Ð´Ð°"""
        if not text:
            return False
        return bool(self._code_patterns.search(text))

    @asynccontextmanager
    async def _create_session(self):
        """Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ aiohttp ÑÐµÑÑÐ¸Ð¸ Ñ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ð¾Ð¼"""
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        session = aiohttp.ClientSession(timeout=timeout)
        try:
            yield session
        finally:
            await session.close()

    async def generate_answer_streaming(self, 
                                    question: str, 
                                    context_docs: List[str],
                                    deep_think: bool = False) -> AsyncGenerator[str, None]:
        """Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° ÑÐ¾ ÑÑ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³Ð¾Ð¼ Ñ tracing"""
        
        logger.info("ðŸ”§ [LLM-1] ÐÐ°Ñ‡Ð°Ð»Ð¾ generate_answer_streaming")
        
        # Ð—Ð°Ñ‰Ð¸Ñ‚Ð½Ñ‹Ð¹ ÑÐ»Ð¾Ð¹: Ð¾Ñ‚ÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÐ¼ Ð² Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¸ÑÐ¿Ð¾Ð»Ð½ÑÐµÐ¼Ð¾Ð³Ð¾ ÐºÐ¾Ð´Ð°/SQL
        if self._is_code_request(question):
            logger.info("ðŸ”§ [LLM-1a] Ð—Ð°Ð¿Ñ€Ð¾Ñ Ð·Ð°Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½ (ÐºÐ¾Ð´/SQL)")
            yield "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ñ Ð½Ðµ Ð¼Ð¾Ð³Ñƒ Ð¿Ð¾Ð¼Ð¾Ð³Ð°Ñ‚ÑŒ Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸ÐµÐ¹ Ð¸ÑÐ¿Ð¾Ð»Ð½ÑÐµÐ¼Ð¾Ð³Ð¾ ÐºÐ¾Ð´Ð° Ð¸Ð»Ð¸ SQL-Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ð¿Ð¾ ÑÐ¾Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸ÑÐ¼ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ð¾ÑÑ‚Ð¸."
            return

        logger.info("ðŸ”§ [LLM-2] Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚")
        prompt = self._create_prompt(question, context_docs, deep_think)
        
        logger.info("ðŸ”§ [LLM-3] ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÑ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³ Ð¾Ñ‚ Ollama")
        try:
            chunk_count = 0
            async for chunk in self._stream_from_ollama(prompt):
                logger.info(f"ðŸ”§ [LLM-3a] ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ chunk #{chunk_count}: '{chunk}'")
                chunk_count += 1
                yield chunk
                    
            logger.info(f"ðŸ”§ [LLM-4] generate_answer_streaming Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½. Ð§Ð°Ð½ÐºÐ¾Ð²: {chunk_count}")
            
        except Exception as e:
            logger.error(f"ðŸ”§ [LLM-ERROR] ÐžÑˆÐ¸Ð±ÐºÐ° Ð² generate_answer_streaming: {e}")
            yield self._fallback_answer(context_docs)

    async def _stream_from_ollama(self, prompt: str) -> AsyncGenerator[str, None]:
        """ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ñ‚Ð¾ÐºÐ¾Ð²Ð¾Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ Ollama API"""
        url = f"{self.base_url}/api/generate"
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": True,
            "options": {
                "temperature": 0.1,
                "top_p": 0.9,
                "num_predict": 500,
                "repeat_penalty": 1.2
            }
        }
        
        logger.info(f"ðŸ”§ ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ðº Ollama...")
        
        async with self._create_session() as session:
            try:
                async with session.post(url, json=payload) as response:
                    logger.info(f"ðŸ”§ Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {response.status}")
                    
                    if response.status != 200:
                        error_text = await response.text()
                        logger.error(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ollama API: {response.status} - {error_text}")
                        yield "âŒ ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾ÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ."
                        return
                    
                    logger.info("ðŸ”§ ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ Ñ‡Ñ‚ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ñ‚Ð¾ÐºÐ°...")
                    
                    # Ð§Ð¸Ñ‚Ð°ÐµÐ¼ Ð¿Ð¾Ñ‚Ð¾Ðº Ð¿Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ð¾
                    async for line_bytes in response.content:
                        if line_bytes:
                            line = line_bytes.decode('utf-8').strip()
                            
                            # ÐŸÑ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÑ‚Ñ€Ð¾ÐºÐ¸
                            if not line:
                                continue
                                
                            try:
                                data = json.loads(line)
                                
                                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ðµ
                                if data.get('done', False):
                                    logger.info("ðŸ”§ Ð¡Ñ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½")
                                    break
                                
                                # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ response ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
                                if 'response' in data and data['response']:
                                    yield data['response']
                                    
                            except json.JSONDecodeError:
                                logger.warning(f"ðŸ”§ ÐÐµÐ²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON: {line}")
                                continue
                            except Exception as e:
                                logger.warning(f"ðŸ”§ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {e}")
                                continue
                    
                    logger.info("ðŸ”§ ÐŸÐ¾Ñ‚Ð¾Ðº Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½")
                                    
            except asyncio.TimeoutError:
                logger.error("â° Ð¢Ð°Ð¹Ð¼Ð°ÑƒÑ‚ Ð¿Ñ€Ð¸ ÑÑ‚Ñ€Ð¸Ð¼Ð¸Ð½Ð³Ðµ")
                yield "â° ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¾ Ð²Ñ€ÐµÐ¼Ñ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°."
            except aiohttp.ClientConnectorError:
                logger.error("ðŸ”Œ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒÑÑ Ðº Ollama")
                yield "ðŸ”Œ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡Ð¸Ñ‚ÑŒÑÑ Ðº ÑÐ·Ñ‹ÐºÐ¾Ð²Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸."
            except Exception as e:
                logger.error(f"ðŸ’¥ ÐÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {e}")
                yield "âŒ ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð°."

    async def generate_answer(self, 
                            question: str, 
                            context_docs: List[str],
                            deep_think: bool = False) -> str:
        """ÐžÐ±Ñ‹Ñ‡Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° (Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ¾Ð²Ð¼ÐµÑÑ‚Ð¸Ð¼Ð¾ÑÑ‚Ð¸)"""
        full_response = ""
        async for chunk in self.generate_answer_streaming(question, context_docs, deep_think):
            full_response += chunk
        return full_response

    def _create_prompt(self, 
                    question: str, 
                    context_docs: List[str],
                    deep_think: bool) -> str:
        """Ð£Ð¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ"""
        
        context_text = "\n".join(context_docs) if context_docs else "Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°"
        
        prompt = f"""ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ: {question}

    Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð°:
    {context_text}

    ÐžÑ‚Ð²ÐµÑ‚:"""
        
        return prompt
    
    def _fallback_answer(self, context_docs: List[str]) -> str:
        """ÐžÑ‚Ð²ÐµÑ‚ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ LLM"""
        if not context_docs:
            return "âŒ Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¿Ð¾ Ð²Ð°ÑˆÐµÐ¼Ñƒ Ð·Ð°Ð¿Ñ€Ð¾ÑÑƒ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° Ð² Ð±Ð°Ð·Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹."
        
        return "ÐÐ°Ð¹Ð´ÐµÐ½Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ:\n" + "\n".join(
            [f"â€¢ {doc[:100]}..." if len(doc) > 100 else f"â€¢ {doc}" 
             for doc in context_docs[:3]]
        )