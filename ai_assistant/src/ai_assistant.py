"""–û—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥—É–ª—å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
from typing import List, Optional, Dict, Any, AsyncGenerator
import asyncio
import logging
import os
import time

# –ò–º–ø–æ—Ä—Ç—ã –∏–∑ —Ç–æ–π –∂–µ –ø–∞–ø–∫–∏
from .cache_manager import EmbeddingCache
from .config_manager import ConfigManager
from .security_checker import SecurityChecker
from .metrics_collector import MetricsCollector
from .dialogue_memory import DialogueMemory
from .llm_adapter import LLMAdapter, LLMError
from .embeddings_manager import EmbeddingsManager
from .stock_analyzer import StockAnalyzer

logger = logging.getLogger(__name__)

class AssistantInitializationError(Exception):
    """–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    pass

class SmartDeepThinkRAG:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π streaming"""
    
    def __init__(self, config_path: str = "config.json"):
        try:
            self.config = ConfigManager.load_config(config_path)
            self._validate_config()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
            self.embedding_manager = EmbeddingsManager(
                self.config['embedder']['model_name']
            )
            self.embedding_cache = EmbeddingCache()
            self.llm = LLMAdapter(
                model_name=self.config['model']['name'],
                timeout=self.config.get('model', {}).get('timeout', 120)
            )
            self.security = SecurityChecker()
            self.metrics = MetricsCollector()
            self.memory = DialogueMemory()
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –∞–∫—Ü–∏–π
            self.stock_analyzer = StockAnalyzer()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
            self.documents = self._load_knowledge_base()
            self.doc_embeddings = self.embedding_manager.precompute_embeddings(
                self.documents, 
                self.embedding_cache
            )
            
            logger.info(f"–°–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ: {len(self.documents)}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {e}")
            raise AssistantInitializationError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç: {e}")

    def _validate_config(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        required_fields = ['model', 'rag', 'embedder']
        for field in required_fields:
            if field not in self.config:
                raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {field}")

    def _load_knowledge_base(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞"""
        knowledge_base_paths = [
            "knowledge_base.txt",
            os.path.join(os.path.dirname(__file__), '..', 'data', 'knowledge_base.txt'),
            os.path.join(os.path.dirname(__file__), 'knowledge_base.txt')
        ]
        
        for kb_path in knowledge_base_paths:
            try:
                with open(kb_path, 'r', encoding='utf-8') as f:
                    documents = [line.strip() for line in f if line.strip()]
                    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∏–∑ {kb_path}: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
                    return documents
            except FileNotFoundError:
                continue
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π {kb_path}: {e}")
                continue
                
        logger.warning("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return []

    async def ask_streaming(self, question: str) -> AsyncGenerator[str, None]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º DeepThink –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π –∞–∫—Ü–∏–π"""
        start_time = time.time()
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–ª–∞–≥–∏ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–∂–∏–º—ã
            clean_question, flags = self.security._extract_flags(question)
            deepthink_mode = question.endswith(" -deepthink") and '-nodeep' not in flags
            
            if deepthink_mode:
                clean_question = clean_question[:-10].strip()
                yield "–ê–ö–¢–ò–í–ò–†–û–í–ê–ù –†–ï–ñ–ò–ú DEEPTHINK\n"
                yield "=" * 50 + "\n"
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            is_safe, reason = await self.security.check(question)
            if not is_safe:
                if deepthink_mode:
                    yield f"–ê–ù–ê–õ–ò–ó –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò: {reason}\n"
                    yield "–ó–∞–ø—Ä–æ—Å –æ—Ç–∫–ª–æ–Ω–µ–Ω –ø–æ –ø–æ–ª–∏—Ç–∏–∫–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏\n"
                    yield "=" * 50 + "\n"
                else:
                    yield reason
                return
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–≤–µ—Ç–∞
            question_embedding = await self.embedding_manager.get_embedding(clean_question)
            similar_docs = await self.embedding_manager.find_similar(
                clean_question, question_embedding, self.doc_embeddings, self.documents, top_k=3
            )
            
            # –ê–Ω–∞–ª–∏–∑ –∞–∫—Ü–∏–π (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
            investment_analysis = None
            question_lower = clean_question.lower()
            
            if any(word in question_lower for word in ['–∞–∫—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–≤–ª–æ–∂–∏—Ç—å', '–∫—É–¥–∞ –≤–ª–æ–∂–∏—Ç—å', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '–≤—ã–≥–æ–¥–Ω']):
                market_data = await self._get_real_market_data()
                
                # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∞–∫—Ü–∏–∏
                for symbol in ['GAZP', 'SBER', 'LKOH', 'YNDX', 'ROSN', 'VTBR']:
                    if symbol.lower() in question_lower:
                        investment_analysis = await self.stock_analyzer.analyze_single_stock(symbol, market_data)
                        break
                
                # –û–±—â–∏–π –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                if not investment_analysis:
                    investment_analysis = await self.stock_analyzer.analyze_investment_query(clean_question, market_data)
            
            # DeepThink –∞–Ω–∞–ª–∏–∑
            if deepthink_mode:
                yield await self._generate_deepthink_analysis(clean_question, similar_docs, investment_analysis)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ (–µ—Å–ª–∏ –µ—Å—Ç—å)
            if investment_analysis and 'error' not in investment_analysis:
                yield "\n–§–ò–ù–ê–ù–°–û–í–´–ô –ê–ù–ê–õ–ò–ó:\n"
                if 'strategy_name' in investment_analysis:
                    strategy = investment_analysis
                    yield f"{strategy['strategy_name'].upper()} –°–¢–†–ê–¢–ï–ì–ò–Ø\n"
                    yield f"{strategy['strategy_description']}\n"
                    yield f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: {strategy['recommended_allocation']}\n\n"
                    
                    yield "–†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ï –ê–ö–¶–ò–ò:\n"
                    for stock in strategy['stocks']:
                        yield f"‚Ä¢ {stock['name']} ({stock['symbol']}) - {stock['price']} —Ä—É–±.\n"
                        yield f"  –†–∏—Å–∫: {stock['risk']} | –î–∏–≤–∏–¥–µ–Ω–¥—ã: {stock['dividend_yield']}\n"
                        description = stock.get('description', '')
                        if description:
                            yield f"  üìä {description}\n"
                        yield "\n"
                else:
                    # –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π –∞–∫—Ü–∏–∏
                    stock = investment_analysis
                    yield f"{stock['name']} ({stock['symbol']})\n"
                    yield f"–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞: {stock['current_price']} —Ä—É–±.\n"
                    yield f"–î–∏–Ω–∞–º–∏–∫–∞: {stock['change']:+.2f} ({stock['change_percent']:+.2f}%)\n"
                    yield f"–¢—Ä–µ–Ω–¥: {stock['trend']}\n"
                    yield f"–£—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–∞: {stock['risk']}\n"
                    yield f"–î–∏–≤–∏–¥–µ–Ω–¥–Ω–∞—è –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å: {stock['dividend_yield']}\n"
                    yield f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {stock['recommendation']}\n\n"
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
            if deepthink_mode:
                yield "–û–°–ù–û–í–ù–û–ô –û–¢–í–ï–¢:\n"
            elif '-simple' in flags:
                yield "[–ü–†–û–°–¢–û–ô –†–ï–ñ–ò–ú] "
            else:
                yield "–û—Ç–≤–µ—Ç: "
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            full_response = ""
            relevant_chunks = []
            
            async for chunk in self.llm.generate_answer_streaming(clean_question, similar_docs, deepthink_mode, flags):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —á–∞–Ω–∫–∞
                if self._is_relevant_chunk(chunk, clean_question):
                    relevant_chunks.append(chunk)
                    yield chunk
                full_response += chunk
            
            # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω - –¥–∞–µ–º –∑–∞–ø–∞c–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç
            if not self._is_response_relevant(full_response, clean_question) and investment_analysis:
                yield "\n\n–ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é:\n"
                if 'stocks' in investment_analysis:
                    for stock in investment_analysis['stocks'][:3]:
                        yield f"‚Ä¢ {stock['name']} - {stock.get('price', 'N/A')} —Ä—É–±. ({stock.get('risk', 'N/A')} —Ä–∏—Å–∫)\n"
                elif 'current_price' in investment_analysis:
                    stock = investment_analysis
                    yield f"‚Ä¢ {stock['name']} - {stock.get('current_price', 'N/A')} —Ä—É–±. ({stock.get('risk', 'N/A')} —Ä–∏—Å–∫)\n"
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø–∞–º—è—Ç—å –∏ –≤—ã–≤–æ–¥–∏–º –≤—Ä–µ–º—è
            self.memory.add_message('user', clean_question)
            self.memory.add_message('assistant', full_response)
            
            response_time = time.time() - start_time
            yield f"\n\n‚è±–í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response_time:.2f} —Å–µ–∫"
            
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ ask_streaming: {e}", exc_info=True)
            yield f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}"

    async def _generate_deepthink_analysis(self, question: str, similar_docs: List[str], investment_analysis: Any) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è DeepThink —Ä–µ–∂–∏–º–∞"""
        analysis = []
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        question_lower = question.lower()
        if any(word in question_lower for word in ['—á—Ç–æ —Ç–∞–∫–æ–µ', '–æ–ø—Ä–µ–¥–µ–ª']):
            intent = "–ü–û–õ–£–ß–ò–¢–¨ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï"
        elif any(word in question_lower for word in ['–∫–∞–∫', '–ø—Ä–æ—Ü–µ—Å—Å']):
            intent = "–£–ó–ù–ê–¢–¨ –ü–†–û–¶–ï–°–°"
        elif any(word in question_lower for word in ['–¥–æ–∫—É–º–µ–Ω—Ç', '–Ω—É–∂–Ω–æ']):
            intent = "–£–ó–ù–ê–¢–¨ –î–û–ö–£–ú–ï–ù–¢–´" 
        elif any(word in question_lower for word in ['—Å—Ç–∞–≤–∫', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']):
            intent = "–£–ó–ù–ê–¢–¨ –°–¢–û–ò–ú–û–°–¢–¨"
        elif any(word in question_lower for word in ['–∞–∫—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–≤–ª–æ–∂–∏—Ç—å']):
            intent = "–ò–ù–í–ï–°–¢–ò–¶–ò–û–ù–ù–´–ô –ó–ê–ü–†–û–°"
        else:
            intent = "‚Ñπ–û–ë–©–ò–ô –ó–ê–ü–†–û–°"
        
        analysis.append(f"–ê–ù–ê–õ–ò–ó –ù–ê–ú–ï–†–ï–ù–ò–Ø: {intent}")
        analysis.append(f"–û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô –í–û–ü–†–û–°: '{question}'")
        analysis.append(f"–ù–ê–ô–î–ï–ù–û –î–û–ö–£–ú–ï–ù–¢–û–í: {len(similar_docs)}")
        
        if similar_docs:
            analysis.append("–†–ï–õ–ï–í–ê–ù–¢–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:")
            for i, doc in enumerate(similar_docs[:2], 1):
                preview = doc[:100] + "..." if len(doc) > 100 else doc
                analysis.append(f"   {i}. {preview}")
        
        if investment_analysis and 'error' not in investment_analysis:
            analysis.append("–î–û–°–¢–£–ü–ù–´ –î–ê–ù–ù–´–ï –†–´–ù–ö–ê:")
        
        analysis.append("–°–û–û–¢–í–ï–¢–°–¢–í–ò–ï –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò: –ü—Ä–æ–≤–µ—Ä–µ–Ω–æ")
        analysis.append("=" * 50)
        analysis.append("")
        
        return "\n".join(analysis)

    async def _get_real_market_data(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            from ..parsers.financial_parser import FinancialDataParser
            
            parser = FinancialDataParser()
            market_data = {}
            
            # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –∞–∫—Ü–∏—è–º
            symbols = ['SBER', 'GAZP', 'LKOH', 'YNDX', 'ROSN', 'VTBR']
            
            for symbol in symbols:
                try:
                    stock_data = await parser.get_stock_price(symbol)
                    if 'error' not in stock_data:
                        market_data[symbol] = {
                            'last_price': stock_data.get('last_price'),
                            'change': stock_data.get('change', 0),
                            'change_percent': stock_data.get('change_percent', 0),
                            'volume': stock_data.get('volume', 0)
                        }
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ {symbol}: {e}")
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–ª—É—à–∫—É –µ—Å–ª–∏ API –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–æ
                    market_data[symbol] = await self._get_fallback_data(symbol)
            
            await parser.close()
            return market_data
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
            return await self._get_fallback_data()
        

    def _is_relevant_chunk(self, chunk: str, question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —á–∞–Ω–∫–∞ –≤–æ–ø—Ä–æ—Å—É"""
        question_lower = question.lower()
        chunk_lower = chunk.lower()
        
        # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –ø—Ä–æ –∞–∫—Ü–∏–∏, –∞ –æ—Ç–≤–µ—Ç –ø—Ä–æ –∏–ø–æ—Ç–µ–∫—É - –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ
        if any(word in question_lower for word in ['–∞–∫—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–≤–ª–æ–∂–∏—Ç—å']):
            if any(word in chunk_lower for word in ['–∏–ø–æ—Ç–µ–∫', '–∫—Ä–µ–¥–∏—Ç –Ω–∞ –Ω–µ–¥–≤–∏–∂–∏–º–æ—Å—Ç—å', '–≤–∫–ª–∞–¥']):
                return False
        
        return True

    def _is_response_relevant(self, response: str, question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –≤—Å–µ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        question_lower = question.lower()
        response_lower = response.lower()
        
        relevant_keywords = []
        if any(word in question_lower for word in ['–∞–∫—Ü–∏–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü']):
            relevant_keywords = ['–∞–∫—Ü–∏', '—Å–±–µ—Ä', '–≥–∞–∑–ø—Ä–æ–º', '–ª—É–∫–æ–π–ª', '—è–Ω–¥–µ–∫—Å', '–¥–∏–≤–∏–¥–µ–Ω—Ç', '–ø–æ—Ä—Ç—Ñ–µ–ª—å', '–∏–Ω–≤–µ—Å—Ç']
        
        return any(keyword in response_lower for keyword in relevant_keywords)

    async def ask(self, question: str) -> str:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–∑ main.py"""
        full_response = ""
        async for chunk in self.ask_streaming(question):
            print(chunk, end='', flush=True)
            full_response += chunk
        print()  # –ö–æ–Ω–µ—á–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
        return full_response
    
    async def ask_streaming_wrapper(self, question: str) -> None:
        """–û–±–µ—Ä—Ç–∫–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –Ω–∞–ø—Ä—è–º—É—é –≤ –∫–æ–Ω—Å–æ–ª—å"""
        async for chunk in self.ask_streaming(question):
            print(chunk, end='', flush=True)
        print()  # –ö–æ–Ω–µ—á–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏

    def ask_sync(self, question: str) -> str:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ"""
        import asyncio
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                future = asyncio.run_coroutine_threadsafe(self.ask(question), loop)
                return future.result()
            else:
                return loop.run_until_complete(self.ask(question))
        except RuntimeError:
            return asyncio.run(self.ask(question))

    def _fix_keyboard_layout(self, text: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –Ω–∞–±—Ä–∞–Ω–Ω–æ–≥–æ –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π —Ä–∞—Å–∫–ª–∞–¥–∫–µ –≤–º–µ—Å—Ç–æ —Ä—É—Å—Å–∫–æ–π"""
        if not text:
            return text

        en_chars = "qwertyuiop[]asdfghjkl;'zxcvbnm,./`"
        ru_chars = "–π—Ü—É–∫–µ–Ω–≥—à—â–∑—Ö—ä—Ñ—ã–≤–∞–ø—Ä–æ–ª–¥–∂—ç—è—á—Å–º–∏—Ç—å–±—é.—ë"
        en_upper = en_chars.upper()
        ru_upper = ru_chars.upper()

        char_map = str.maketrans(en_chars + en_upper, ru_chars + ru_upper)

        latin_count = sum(1 for c in text if c in en_chars + en_upper)
        total_len = len(text)
        latin_ratio = (latin_count / total_len) if total_len > 0 else 0.0

        if latin_ratio > 0.3:
            fixed = text.translate(char_map)
            logger.info("–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ —Ä–∞—Å–∫–ª–∞–¥–∫–∞: '%s' -> '%s'", text, fixed)
            return fixed

        return text

# –ï—Å–ª–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç        
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    try:
        assistant = SmartDeepThinkRAG()
        
        print("\n–ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        print("–î–ª—è —É–≥–ª—É–±–ª–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–±–∞–≤—å—Ç–µ '-deepthink' –∫ –≤–æ–ø—Ä–æ—Å—É")
        print("–î–ª—è –≤—ã—Ö–æ–¥–∞ –≤–≤–µ–¥–∏—Ç–µ 'exit', 'quit' –∏–ª–∏ '—Å—Ç–æ–ø'")
        
        while True:
            question = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
            if question.lower() in ['exit', 'quit', '—Å—Ç–æ–ø']:
                print("–î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            if not question:
                continue
                
            assistant.ask_sync(question)
            
    except AssistantInitializationError as e:
        print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
        print("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
    except KeyboardInterrupt:
        print("\n–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")