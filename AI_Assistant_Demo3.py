import json
import subprocess
import sys
import numpy as np
import time
import hashlib
import pickle
import os
import asyncio
import weakref
from langchain_ollama import OllamaLLM
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import logging
from functools import lru_cache
from dotenv import load_dotenv
from prometheus_client import Counter, Histogram

logger = logging.getLogger(__name__)

class EmbeddingCache:
    def __init__(self, cache_file='embeddings_cache.pkl'):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self):
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return {}
    
    def _save_cache(self):
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
    
    def get_embedding(self, text, embedder):
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        embedding = embedder.encode([text])[0]
        self.cache[text_hash] = embedding
        self._save_cache()
        return embedding

    

class MetricsCollector:
    def __init__(self):
        self.request_counter = Counter('assistant_requests_total', 'Total requests')
        self.response_time = Histogram('assistant_response_seconds', 'Response time')
        self.metrics = {
            'total_queries': 0,
            'successful_responses': 0,
            'security_blocks': 0,
            'avg_response_time': 0,
            'intent_distribution': {}
        }
    
    def log_query(self, question, intent, response_time, success=True):
        self.metrics['total_queries'] += 1
        if success:
            self.metrics['successful_responses'] += 1
        self.metrics['intent_distribution'][intent] = self.metrics['intent_distribution'].get(intent, 0) + 1
        self.metrics['avg_response_time'] = (
            self.metrics['avg_response_time'] * (self.metrics['total_queries'] - 1) + response_time
        ) / self.metrics['total_queries']
    
    def get_metrics(self):
        return self.metrics.copy()

class DialogueMemory:
    def __init__(self, max_messages=10):
        self.messages = []
        self.max_messages = max_messages
    
    def add_message(self, role, content):
        self.messages.append({'role': role, 'content': content, 'timestamp': time.time()})
        if len(self.messages) > self.max_messages:
            self.messages.pop(0)
    
    def get_context(self, window_minutes=30):
        cutoff_time = time.time() - (window_minutes * 60)
        return [msg for msg in self.messages if msg['timestamp'] > cutoff_time]

class ConfigManager:
    @staticmethod
    def load_config(filename, default=None):
        """–ó–∞–≥—Ä—É–∑–∫–∞ JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É—é –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")
            return default if default else {}
        except json.JSONDecodeError as e:
            print(f"–û—à–∏–±–∫–∞ –≤ —Ñ–∞–π–ª–µ {filename}: {e}")
            return default if default else {}

    def __init__(self):
        load_dotenv()
        self.model_name = os.getenv('MODEL_NAME', 'qwen2.5:0.5b')
        self.temperature = float(os.getenv('TEMPERATURE', '0.1'))

class DependencyManager:
    @staticmethod
    def check_and_install():
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –í–°–ï–• –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π"""
        required_packages = [
            "sentence-transformers", 
            "requests", 
            "numpy", 
            "langchain-ollama",
            "langchain-core",
            "scikit-learn"
        ]
        
        # –ü–æ–ø—ã—Ç–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
        try:
            print("–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π...")
            subprocess.check_call([
                sys.executable, "-m", "pip", "install", "-q"
            ] + required_packages)
            print("‚úÖ –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã!")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –º–∞—Å—Å–æ–≤–æ–π —É—Å—Ç–∞–Ω–æ–≤–∫–∏, –ø—Ä–æ–±—É—é –ø–æ –æ–¥–Ω–æ–π: {e}")
            # –§–æ–ª–±—ç–∫ –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É –ø–æ –æ–¥–Ω–æ–π
            for package in required_packages:
                try:
                    __import__(package.replace("-", "_"))
                    print(f"–ü–∞–∫–µ—Ç {package} —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                except ImportError:
                    print(f"–£—Å—Ç–∞–Ω–æ–≤–∫–∞ {package}...")
                    try:
                        subprocess.check_call([sys.executable, "-m", "pip", "install", "-q", package])
                        print(f"‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {package}")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ {package}: {e}")

from sentence_transformers import SentenceTransformer
from langchain_ollama import OllamaLLM

class SmartDeepThinkRAG:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫"""
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SmartDeepThinkRAG...")
        self._setup_logging()
        
        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
        self.config = ConfigManager.load_config('config.json', {
            'model': {'name': 'qwen2.5:0.5b', 'temperature': 0.1},
            'rag': {'top_k_documents': 3}
        })
        self.intents = ConfigManager.load_config('intents.json', {}) 
        self.security_rules = ConfigManager.load_config('security_rules.json', {})
        self.reasoning_templates = ConfigManager.load_config('reasoning_templates.json', {})
        
        # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        self.documents = self._load_knowledge_base()
        if not self.documents:
            print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞!")
        
        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        try:
            self.embedder = SentenceTransformer('cointegrated/rubert-tiny2')
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ embedder: {e}")
            self.embedder = None
            
        # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º
        self.embedding_cache = EmbeddingCache()
        self.metrics = MetricsCollector()
        
        # 5. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞
        if self.embedder and self.documents:
            print("üìä –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self.doc_embeddings = np.array([
                self.embedding_cache.get_embedding(doc, self.embedder) 
                for doc in self.documents
            ])
        else:
            self.doc_embeddings = np.array([])
        
        # 6. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM
        self.llm = self._init_llm()
        
        # 7. –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = [
            f"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {len(self.documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤",
            f"üíæ –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(self.embedding_cache.cache)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤",
            f"ü§ñ LLM: {'–≥–æ—Ç–æ–≤–∞' if self.llm else '–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}"
        ]
        print("\n".join(["‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞:"] + stats))
    
    def _load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            with open('knowledge_base.txt', 'r', encoding='utf-8') as f:
                documents = [line.strip() for line in f if line.strip() and ':' in line]
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}")
            return documents
        except FileNotFoundError:
            print("–§–∞–π–ª knowledge_base.txt –Ω–µ –Ω–∞–π–¥–µ–Ω.")
            return []
    
    def _init_llm(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        model_config = self.config.get('model', {})
        try:
            llm = OllamaLLM(
                model=model_config.get('name', 'qwen2.5:0.5b'),
                temperature=model_config.get('temperature', 0.1),
                num_predict=model_config.get('max_tokens', 200)
            )
            print(f"–ú–æ–¥–µ–ª—å {model_config.get('name')} –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return llm
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ LLM: {e}")
            return None
    
    def security_check(self, question):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        dangerous_patterns = self.security_rules.get('dangerous_patterns', {})
        question_lower = question.lower()
        
        for pattern, action in dangerous_patterns.items():
            if pattern in question_lower:
                return False, f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ—Å–∏—Ç –º–µ–Ω—è {action}. –ù–æ —ç—Ç–æ –∑–∞–ø—Ä–µ—â–µ–Ω–æ –º–æ–∏–º–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."
        
        return True, "–ó–∞–ø—Ä–æ—Å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"
    
    def analyze_question_intent(self, question):
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ JSON –∫–æ–Ω—Ñ–∏–≥–∞"""
        question_lower = question.lower()
        
        for intent_name, intent_config in self.intents.items():
            keywords = intent_config.get('keywords', [])
            if any(keyword in question_lower for keyword in keywords):
                return intent_name, intent_config.get('description', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ')
        
        return "general", "–û–±—â–∏–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å"
    
    @lru_cache(maxsize=1000)
    def find_similar_docs(self, question: str) -> List[str]:
        """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        rag_config = self.config.get('rag', {})
        top_k = rag_config.get('top_k_documents', 3)
        
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ –∫—ç—à
        question_embedding = self.embedding_cache.get_embedding(question, self.embedder)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –∏ –ø–æ–ª—É—á–∞–µ–º —Ç–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        similarities = np.dot(self.doc_embeddings, question_embedding)
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        result = [self.documents[i] for i in top_indices]
        
        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        print(f"\nüîç –ü–æ–∏—Å–∫: '{question}'")
        print(f"üìë –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(result)}")
        for i, doc in enumerate(result, 1):
            similarity = similarities[top_indices[i-1]]
            print(f"   {i}. (score: {similarity:.2f}) {doc[:80]}...")
        
        return result
    
    def _precompute_embeddings(self):
        return [self.embedding_cache.get_embedding(doc, self.embedder) for doc in self.documents]

    def generate_llm_answer(self, question, context_docs):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Qwen —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∞–º–∏"""
        if not context_docs:
            return "‚ùå –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
        
        if not self.llm:
            return self._fallback_answer(question, context_docs)
        
        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ü–†–û–ú–ü–¢
        prompt = f"""–¢—ã - –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –±–∞–Ω–∫–∞. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –∏—Å–ø–æ–ª—å–∑—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ.

–ö–û–ù–¢–ï–ö–°–¢:
{chr(10).join(context_docs)}

–í–û–ü–†–û–°: {question}

–ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
2. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ñ–∞–∫—Ç—ã –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ - —Å–∫–∞–∂–∏ "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
4. –ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º –∏ —Ç–æ—á–Ω—ã–º
5. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
6. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–æ–ª–Ω—ã–º –∏ –ª–æ–≥–∏—á–µ—Å–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã–º
7. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ - 2000 —Å–∏–º–≤–æ–ª–æ–≤

–û–¢–í–ï–¢:"""
    
        try:
            response = self.llm.invoke(prompt)
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
            if len(response.strip()) < 10:
                return self._fallback_answer(question, context_docs)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—Ä—ã–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
            if response[-1] not in '.!?':
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ–ª–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
                last_sentence = response.rsplit('.', 1)[0] + '.'
                response = last_sentence
                
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—Ä—ã–≤ —Å–ø–∏—Å–∫–∞
            if response.count('\n1.') > 0:  # –ï—Å—Ç—å –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
                lines = response.split('\n')
                complete_lines = []
                current_number = 1
                
                for line in lines:
                    if line.strip().startswith(f"{current_number}."):
                        complete_lines.append(line)
                        current_number += 1
                    elif not line.strip().startswith(f"{current_number}.") and current_number > 1:
                        break
                    else:
                        complete_lines.append(line)
                
                response = '\n'.join(complete_lines)
                
            return response
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
            return self._fallback_answer(question, context_docs)
    
    def _fallback_answer(self, question, context_docs):
        """–§–æ–ª–±—ç–∫ –æ—Ç–≤–µ—Ç –µ—Å–ª–∏ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞"""
        intent, _ = self.analyze_question_intent(question)
        
        if intent == "definition":
            for doc in context_docs:
                if ':' in doc:
                    product, definition = doc.split(':', 1)
                    if any(keyword in question.lower() for keyword in product.lower().split()):
                        return f"{product.strip()} - —ç—Ç–æ {definition.strip()}"
        
        # –ü—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        products_info = []
        for doc in context_docs[:2]:
            if ':' in doc:
                product, info = doc.split(':', 1)
                products_info.append(f"- {product.strip()}: {info.strip()}")
        
        return "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É:\n" + "\n".join(products_info) if products_info else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"
    
    def deep_think_process(self, question, context_docs):
        """–ü—Ä–æ—Ü–µ—Å—Å –º—ã—à–ª–µ–Ω–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º JSON —à–∞–±–ª–æ–Ω–æ–≤"""
        thinking = []
        templates = self.reasoning_templates.get('thinking_process', {})
        
        thinking.append(templates.get('header', '–ü–†–û–¶–ï–°–° –ú–´–®–õ–ï–ù–ò–Ø:'))
        thinking.append(templates.get('user_question', '').format(question=question))
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        intent, intent_description = self.analyze_question_intent(question)
        thinking.append(templates.get('intent_analysis', '').format(intent_description=intent_description))
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        is_safe, security_reason = self.security_check(question)
        thinking.append(templates.get('security_check', '').format(security_reason=security_reason))
        
        if not is_safe:
            thinking.append(templates.get('rejection', ''))
            return "\n".join(thinking), None
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        thinking.append(templates.get('documents_found', '').format(doc_count=len(context_docs)))
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        thinking.append("\n–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∏:")
        analysis_templates = self.reasoning_templates.get('analysis_templates', {})
        
        if intent in analysis_templates:
            for line_template in analysis_templates[intent]:
                if '{products}' in line_template:
                    product_keywords = ['–∫—Ä–µ–¥–∏—Ç', '–∏–ø–æ—Ç–µ–∫', '–≤–∫–ª–∞–¥', '–∫–∞—Ä—Ç', '—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω']
                    found_products = [p for p in product_keywords if p in question.lower()]
                    thinking.append("   " + line_template.format(products=', '.join(found_products)))
                else:
                    thinking.append("   " + line_template)
        else:
            for line in analysis_templates.get('general', []):
                thinking.append("   " + line)
        
        thinking.append("\n–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:")
        thinking.append(f"   –û—Ç–æ–±—Ä–∞–ª {len(context_docs)} –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
        return "\n".join(thinking), context_docs
    
    async def _get_embeddings(self, text):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ embedder, –∏—Å–ø–æ–ª—å–∑—É—è executor –∏ –∫—ç—à"""
        if not getattr(self, 'embedder', None) or not getattr(self, 'embedding_cache', None):
            raise LLMError("Embedder –∏–ª–∏ embedding_cache –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        loop = asyncio.get_running_loop()
        # –í—ã–Ω–µ—Å–µ–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ executor
        return await loop.run_in_executor(None, lambda: self.embedding_cache.get_embedding(text, self.embedder))

    async def ask(self, question: str) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥"""
        # –°–Ω–∞—á–∞–ª–∞ –∏–∑–≤–ª–µ–∫–∞–µ–º —Ñ–ª–∞–≥ -deepthink –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–≤–æ–¥–∞, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ä–µ–¥–∏—Ç—å –µ–≥–æ –∞–≤—Ç–æ–∑–∞–º–µ–Ω–æ–π
        deepthink_suffix = ' -deepthink'
        deepthink_mode = False
        raw = question
        if isinstance(raw, str) and raw.lower().rstrip().endswith(deepthink_suffix):
            deepthink_mode = True
            raw = raw[: -len(deepthink_suffix)]
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ä–∞—Å–∫–ª–∞–¥–∫—É —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —á–∞—Å—Ç–∏ (–±–µ–∑ —Ñ–ª–∞–≥–∞)
        fixed_raw = self._fix_keyboard_layout(raw)
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å —Å —Ñ–ª–∞–≥–æ–º, –µ—Å–ª–∏ –æ–Ω –±—ã–ª
        question = (fixed_raw + deepthink_suffix) if deepthink_mode else fixed_raw
        question = question.strip()
        
        start_time = time.time()
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –ø–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏ –≤—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        embedding_task = asyncio.create_task(self._get_embeddings(question))
        security_task = asyncio.create_task(asyncio.to_thread(self.security_check, question))
        
        try:
            question_embedding = await embedding_task
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            raise LLMError("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥")
        
        is_safe, reason = await security_task
        if not is_safe:
            response_time = time.time() - start_time
            intent, _ = self.analyze_question_intent(question.replace(deepthink_suffix, '').strip())
            self.metrics.log_query(question, intent, response_time, success=False)
            return reason
        
        # –ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤, –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ to_thread –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
        context_docs = self.find_similar_docs(question.replace(deepthink_suffix, '').strip())
        
        if deepthink_mode:
            thinking_process, relevant_docs = await asyncio.to_thread(self.deep_think_process, question.replace(deepthink_suffix, '').strip(), context_docs)
            if relevant_docs is None:
                response_time = time.time() - start_time
                intent, _ = self.analyze_question_intent(question.replace(deepthink_suffix, '').strip())
                self.metrics.log_query(question, intent, response_time, success=False)
                return self.security_rules.get('rejection_messages', {}).get('default', '–ó–∞–ø—Ä–æ—Å –æ—Ç–∫–ª–æ–Ω–µ–Ω.')
            answer = await asyncio.to_thread(self.generate_llm_answer, question.replace(deepthink_suffix, '').strip(), relevant_docs)
        else:
            answer = await asyncio.to_thread(self.generate_llm_answer, question.replace(deepthink_suffix, '').strip(), context_docs)
        
        response_time = time.time() - start_time
        intent, _ = self.analyze_question_intent(question.replace(deepthink_suffix, '').strip())
        self.metrics.log_query(question, intent, response_time, success=True)
        
        return f"{answer}\n\n‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {response_time:.2f} —Å–µ–∫"
    
    def ask_sync(self, question):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –º–µ—Ç–æ–¥–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º asyncio.run, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–æ—Ä—É—Ç–∏–Ω—ã
        return asyncio.run(self.ask(question))
    
    def _fix_keyboard_layout(self, text):
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞–±—Ä–∞–Ω–Ω–æ–≥–æ –≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞—Å–∫–ª–∞–¥–∫–µ –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã"""
        en_chars = "qwertyuiop[]asdfghjkl;'zxcvbnm,./`&?"
        ru_chars = "–π—Ü—É–∫–µ–Ω–≥—à—â–∑—Ö—ä—Ñ—ã–≤–∞–ø—Ä–æ–ª–¥–∂—ç—è—á—Å–º–∏—Ç—å–±—é.—ë–∂—ç"
        en_upper = "QWERTYUIOP{}ASDFGHJKL:\"ZXCVBNM<>?~"
        ru_upper = "–ô–¶–£–ö–ï–ù–ì–®–©–ó–•–™–§–´–í–ê–ü–†–û–õ–î–ñ–≠–Ø–ß–°–ú–ò–¢–¨–ë–Æ,–Å"
        
        # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –∑–∞–º–µ–Ω—ã
        char_map = str.maketrans(
            en_chars + en_upper,
            ru_chars + ru_upper
        )
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –∞–Ω–≥–ª–∏–π—Å–∫–∏–º (–Ω–∞–±—Ä–∞–Ω–Ω—ã–º –≤ —Ä—É—Å—Å–∫–æ–π —Ä–∞—Å–∫–ª–∞–¥–∫–µ)
        en_ratio = sum(1 for c in text if c in en_chars + en_upper) / len(text) if text else 0
        
        # –ï—Å–ª–∏ –±–æ–ª–µ–µ 30% —Å–∏–º–≤–æ–ª–æ–≤ - –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–∞–±—Ä–∞–Ω –≤ –Ω–µ–≤–µ—Ä–Ω–æ–π —Ä–∞—Å–∫–ª–∞–¥–∫–µ
        if en_ratio > 0.3:
            fixed_text = text.translate(char_map)
            print(f"\n‚å®Ô∏è –ü–æ—Ö–æ–∂–µ, –≤—ã –Ω–∞–±—Ä–∞–ª–∏ —Ç–µ–∫—Å—Ç –≤ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Ä–∞—Å–∫–ª–∞–¥–∫–µ:")
            print(f"üîÑ –ë—ã–ª–æ:    '{text}'")
            print(f"‚úÖ –°—Ç–∞–ª–æ:   '{fixed_text}'")
            print("üí° –ü–æ–¥—Å–∫–∞–∑–∫–∞: –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ —Ä–∞—Å–∫–ª–∞–¥–∫—É –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –Ω–∞ —Ä—É—Å—Å–∫—É—é (Alt+Shift –∏–ª–∏ Win+Space)\n")
            return fixed_text
        
        return text
    
    def _setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('assistant.log'),
                logging.StreamHandler()
            ]
        )

class MemoryOptimizedCache:
    def __init__(self, max_size: int = 1000):
        self._cache: Dict[str, Any] = weakref.WeakValueDictionary()
        self._max_size = max_size

class AssistantError(Exception):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ—à–∏–±–æ–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    pass

class SecurityError(AssistantError):
    """–û—à–∏–±–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
    pass

class LLMError(AssistantError):
    """–û—à–∏–±–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
    pass

# –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã
if __name__ == "__main__":
    print("–ó–∞–ø—É—Å–∫–∞–µ–º Thinking RAG —Å Qwen...")
    smart_rag = SmartDeepThinkRAG()

    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã:")

    test_cases = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –∫—Ä–µ–¥–∏—Ç? -deepthink",
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –∏–ø–æ—Ç–µ–∫–∏? -deepthink", 
        "–ù–∞–ø–∏—à–∏ –∫–æ–¥ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ SQL -deepthink",
        "–ö–∞–∫–∞—è —Å—Ç–∞–≤–∫–∞ –ø–æ –≤–∫–ª–∞–¥–∞–º? -deepthink",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–µ–±–µ—Ç–æ–≤–∞—è –∫–∞—Ä—Ç–∞? -deepthink",
    ]

    for question in test_cases:
        print(f"\n" + "="*60)
        print(f"–í–æ–ø—Ä–æ—Å: {question}")
        answer = smart_rag.ask_sync(question)
        print(f"\n{answer}")
        print("="*60)

    # –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º
    print("\n–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º")
    print("–î–æ–±–∞–≤—å—Ç–µ '-deepthink' –¥–ª—è –ø–æ–∫–∞–∑–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞ –º—ã—à–ª–µ–Ω–∏—è")
    print("–í–≤–µ–¥–∏—Ç–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ '—Å—Ç–æ–ø' –¥–ª—è –≤—ã—Ö–æ–¥–∞")

    while True:
        user_question = input("\n–í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()
        
        if user_question.lower() in ['—Å—Ç–æ–ø', 'stop', 'exit']:
            print("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
            break
            
        if not user_question:
            continue
        
        answer = smart_rag.ask_sync(user_question)
        print(f"\n{answer}")
        print("-" * 50)