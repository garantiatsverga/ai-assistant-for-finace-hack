# üöÄ –ü–†–û–î–í–ò–ù–£–¢–ê–Ø RAG –°–ò–°–¢–ï–ú–ê –° AI-–§–ò–ß–ê–ú–ò

import re
import time
import pandas as pd
from datetime import datetime
from typing import Dict, List, Tuple
import json
from functools import lru_cache
import subprocess
import os

# ==================== 
# 1. MULTI-AGENT –ê–†–•–ò–¢–ï–ö–¢–£–†–ê
# ====================
class SpecializedAgents:
    def __init__(self, base_rag):
        self.base_rag = base_rag
        self.agents = {
            'credit': self._create_credit_agent(),
            'cards': self._create_cards_agent(), 
            'deposit': self._create_deposit_agent(),
            'insurance': self._create_insurance_agent()
        }
    
    def _create_credit_agent(self):
        return {
            "system_prompt": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫—Ä–µ–¥–∏—Ç–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º. –î–∞–π —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç —Å —Ä–∞—Å—á–µ—Ç–∞–º–∏.",
            "specialization": ["–∫—Ä–µ–¥–∏—Ç", "–∏–ø–æ—Ç–µ–∫–∞", "–∑–∞–µ–º", "—Å—Å—É–¥–∞"],
            "context_boost": 2  # –ò—â–µ–º –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ç–µ–º–µ
        }
    
    def _create_cards_agent(self):
        return {
            "system_prompt": "–¢—ã - —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –∫–∞—Ä—Ç–æ—á–Ω—ã–º –ø—Ä–æ–¥—É–∫—Ç–∞–º. –£–¥–µ–ª–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –∫—ç—à–±—ç–∫—É –∏ –ª–∏–º–∏—Ç–∞–º.",
            "specialization": ["–∫–∞—Ä—Ç–∞", "–∫—ç—à–±—ç–∫", "–ª–∏–º–∏—Ç", "–ø–ª–∞—Ç–µ–∂"],
            "context_boost": 1
        }
    
    def _create_deposit_agent(self):
        return {
            "system_prompt": "–¢—ã - –∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç –ø–æ –≤–∫–ª–∞–¥–∞–º. –†–∞—Å—Å—á–∏—Ç–∞–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å –∏ —Å—Ä–æ–∫–∏.",
            "specialization": ["–≤–∫–ª–∞–¥", "–¥–µ–ø–æ–∑–∏—Ç", "–ø—Ä–æ—Ü–µ–Ω—Ç", "–Ω–∞–∫–æ–ø–ª–µ–Ω–∏–µ"],
            "context_boost": 1
        }
    
    def _create_insurance_agent(self):
        return {
            "system_prompt": "–¢—ã - —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∞–≥–µ–Ω—Ç. –û–±—ä—è—Å–Ω–∏ —É—Å–ª–æ–≤–∏—è –∏ –≤—ã–ø–ª–∞—Ç—ã.",
            "specialization": ["—Å—Ç—Ä–∞—Ö–æ–≤–∞–Ω–∏–µ", "—Å—Ç—Ä–∞—Ö–æ–≤–∫–∞", "–∫–∞—Å–∫–æ", "–æ—Å–∞–≥–æ"],
            "context_boost": 1
        }
    
    def route_question(self, question: str) -> Dict:
        """–ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–∞ –∫ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É"""
        question_lower = question.lower()
        
        for agent_name, agent_config in self.agents.items():
            if any(keyword in question_lower for keyword in agent_config["specialization"]):
                return {
                    "agent": agent_name,
                    "config": agent_config,
                    "confidence": self._calculate_confidence(question_lower, agent_config)
                }
        
        return {"agent": "general", "config": None, "confidence": 1.0}
    
    def _calculate_confidence(self, question: str, agent_config: Dict) -> float:
        """–†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –∞–≥–µ–Ω—Ç—É"""
        matches = sum(1 for keyword in agent_config["specialization"] if keyword in question)
        return min(1.0, matches / len(agent_config["specialization"]))

# ==================== 
# 2. METRICS & QUALITY MONITORING
# ====================
class AdvancedMetrics:
    def __init__(self):
        self.metrics = {
            "response_times": [],
            "cache_performance": {"hits": 0, "misses": 0},
            "agent_usage": {},
            "quality_scores": [],
            "security_events": []
        }
    
    def log_response_time(self, start_time: float):
        time_taken = time.time() - start_time
        self.metrics["response_times"].append(time_taken)
    
    def log_cache_hit(self):
        self.metrics["cache_performance"]["hits"] += 1
    
    def log_cache_miss(self):
        self.metrics["cache_performance"]["misses"] += 1
    
    def log_agent_usage(self, agent_name: str):
        self.metrics["agent_usage"][agent_name] = self.metrics["agent_usage"].get(agent_name, 0) + 1
    
    def calculate_quality_score(self, question: str, answer: str, context: List) -> float:
        """–†–∞—Å—á–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
        scores = []
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        relevance_score = self._calculate_relevance(question, answer)
        scores.append(relevance_score)
        
        # –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã
        completeness_score = self._calculate_completeness(answer)
        scores.append(completeness_score)
        
        # –û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        context_score = self._calculate_context_usage(answer, context)
        scores.append(context_score)
        
        return sum(scores) / len(scores)
    
    def _calculate_relevance(self, question: str, answer: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –≤–æ–ø—Ä–æ—Å—É"""
        question_words = set(question.lower().split())
        answer_words = set(answer.lower().split())
        
        if not question_words:
            return 0.0
            
        overlap = len(question_words.intersection(answer_words))
        return overlap / len(question_words)
    
    def _calculate_completeness(self, answer: str) -> float:
        """–û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –æ—Ç–≤–µ—Ç–∞"""
        ideal_length = 50  # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞
        length = len(answer)
        
        if length < 10:
            return 0.1
        elif length > 500:
            return 0.7
        else:
            return min(1.0, length / ideal_length)
    
    def _calculate_context_usage(self, answer: str, context: List) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not context:
            return 0.0
            
        context_text = " ".join([doc.page_content for doc in context])
        context_words = set(context_text.lower().split())
        answer_words = set(answer.lower().split())
        
        if not context_words:
            return 0.0
            
        overlap = len(context_words.intersection(answer_words))
        return overlap / len(context_words)
    
    def generate_quality_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        avg_time = np.mean(self.metrics["response_times"]) if self.metrics["response_times"] else 0
        cache_hits = self.metrics["cache_performance"]["hits"]
        cache_misses = self.metrics["cache_performance"]["misses"]
        total_requests = cache_hits + cache_misses
        cache_hit_rate = cache_hits / total_requests if total_requests > 0 else 0
        
        quality_avg = np.mean(self.metrics["quality_scores"]) if self.metrics["quality_scores"] else 0
        
        report = f"""
üìä –†–ê–°–®–ò–†–ï–ù–ù–´–ô –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï
{'='*50}
‚è±Ô∏è  –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
   ‚Ä¢ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {avg_time:.2f} —Å–µ–∫
   ‚Ä¢ –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {total_requests}
   ‚Ä¢ Hit-rate –∫—ç—à–∞: {cache_hit_rate:.1%}

üéØ  –ö–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–æ–≤:
   ‚Ä¢ –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {quality_avg:.1%}
   ‚Ä¢ –ö–æ–ª-–≤–æ –æ—Ü–µ–Ω–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤: {len(self.metrics['quality_scores'])}

ü§ñ  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤:
"""
        for agent, count in self.metrics["agent_usage"].items():
            report += f"   ‚Ä¢ {agent}: {count} –∑–∞–ø—Ä–æ—Å–æ–≤\n"
            
        report += f"\nüîí  –°–æ–±—ã—Ç–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {len(self.metrics['security_events'])}"
        
        return report

# ==================== 
# 3. CONTEXT-AWARE PROMPT ENGINE
# ====================
class AdaptivePromptEngine:
    def __init__(self):
        self.conversation_history = []
        self.user_profile = {}
    
    def create_dynamic_prompt(self, question: str, context: List, agent_info: Dict = None) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞
        complexity = self._analyze_complexity(question)
        
        # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        tone = self._analyze_tone(question)
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏
        urgency = self._detect_urgency(question)
        
        base_template = self._select_base_template(complexity, tone, urgency)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–∞
        if agent_info and agent_info.get("agent") != "general":
            specialization = agent_info["config"]["system_prompt"]
            base_template = specialization + "\n\n" + base_template
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        history_context = self._get_conversation_context()
        
        prompt = base_template.format(
            context="\n".join([doc.page_content for doc in context]),
            question=question,
            history=history_context,
            tone=self._get_tone_instruction(tone),
            urgency=self._get_urgency_instruction(urgency)
        )
        
        return prompt
    
    def _analyze_complexity(self, question: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞"""
        word_count = len(question.split())
        complex_indicators = ['—Ä–∞—Å—á–µ—Ç', '—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—É—Å–ª–æ–≤–∏—è', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è']
        
        if word_count > 10 or any(indicator in question.lower() for indicator in complex_indicators):
            return "complex"
        return "simple"
    
    def _analyze_tone(self, question: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –≤–æ–ø—Ä–æ—Å–∞"""
        positive_words = ['—Å–ø–∞—Å–∏–±–æ', '–ø–æ–º–æ–≥–∏—Ç–µ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞']
        negative_words = ['–ø—Ä–æ–±–ª–µ–º–∞', '–∂–∞–ª–æ–±–∞', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç']
        
        if any(word in question.lower() for word in positive_words):
            return "positive"
        elif any(word in question.lower() for word in negative_words):
            return "negative"
        return "neutral"
    
    def _detect_urgency(self, question: str) -> bool:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å—Ä–æ—á–Ω–æ—Å—Ç–∏"""
        urgency_words = ['—Å—Ä–æ—á–Ω–æ', '–±—ã—Å—Ç—Ä–æ', '–Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ', '—Å–µ–π—á–∞—Å']
        return any(word in question.lower() for word in urgency_words)
    
    def _select_base_template(self, complexity: str, tone: str, urgency: bool) -> str:
        """–í—ã–±–æ—Ä –±–∞–∑–æ–≤–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞"""
        
        templates = {
            "simple": """{tone} {urgency}
–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}
–í–æ–ø—Ä–æ—Å: {question}
{history}

–ö—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç:""",

            "complex": """{tone} {urgency}
–í–û–ü–†–û–°: {question}
–ò–°–¢–û–ß–ù–ò–ö–ò: {context}
{history}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:"""
        }
        
        return templates[complexity]
    
    def _get_tone_instruction(self, tone: str) -> str:
        tones = {
            "positive": "–ö–ª–∏–µ–Ω—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω –ø–æ–∑–∏—Ç–∏–≤–Ω–æ. –ë—É–¥—å –æ—Å–æ–±–µ–Ω–Ω–æ –≤–µ–∂–ª–∏–≤—ã–º.",
            "negative": "–ö–ª–∏–µ–Ω—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞—Å—Å—Ç—Ä–æ–µ–Ω. –ü—Ä–æ—è–≤–∏ —ç–º–ø–∞—Ç–∏—é.",
            "neutral": "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω."
        }
        return tones.get(tone, "")
    
    def _get_urgency_instruction(self, urgency: bool) -> str:
        return "‚ùó –ö–õ–ò–ï–ù–¢–£ –¢–†–ï–ë–£–ï–¢–°–Ø –°–†–û–ß–ù–ê–Ø –ü–û–ú–û–©–¨!" if urgency else ""
    
    def _get_conversation_context(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –±–µ—Å–µ–¥—ã"""
        if not self.conversation_history:
            return ""
        
        last_interactions = self.conversation_history[-3:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Ä–µ–ø–ª–∏–∫–∏
        context = "–ü–†–ï–î–´–î–£–©–ê–Ø –ë–ï–°–ï–î–ê:\n"
        
        for i, interaction in enumerate(last_interactions, 1):
            context += f"{i}. –í: {interaction['question'][:50]}...\n"
            context += f"   –û: {interaction['answer'][:50]}...\n"
        
        return context
    
    def update_conversation_history(self, question: str, answer: str):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –±–µ—Å–µ–¥—ã"""
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "question": question,
            "answer": answer
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.conversation_history) > 10:
            self.conversation_history.pop(0)

# ==================== 
# 4. EXPLAINABLE AI (XAI) MODULE
# ====================
class ExplainableAI:
    def __init__(self):
        self.explanation_templates = {
            "source_based": self._source_based_explanation,
            "confidence_based": self._confidence_based_explanation,
            "process_based": self._process_based_explanation
        }
    
    def generate_explanation(self, question: str, answer: str, 
                           sources: List, confidence: float, 
                           agent_info: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞"""
        
        explanations = []
        
        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        explanations.append(self.explanation_templates["source_based"](sources))
        
        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        explanations.append(self.explanation_templates["confidence_based"](confidence))
        
        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞
        explanations.append(self.explanation_templates["process_based"](agent_info))
        
        # –°–±–æ—Ä–∫–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
        final_explanation = f"""
üß† –û–ë–™–Ø–°–ù–ï–ù–ò–ï –û–¢–í–ï–¢–ê

üìã **–í–æ–ø—Ä–æ—Å:** {question}
‚úÖ **–û—Ç–≤–µ—Ç:** {answer}

"""
        
        for explanation in explanations:
            if explanation:
                final_explanation += explanation + "\n\n"
        
        return final_explanation.strip()
    
    def _source_based_explanation(self, sources: List) -> str:
        if not sources:
            return "‚ÑπÔ∏è  –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: –û—Ç–≤–µ—Ç –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏—è—Ö —Å–∏—Å—Ç–µ–º—ã"
        
        explanation = "üîç **–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏:**\n"
        for i, source in enumerate(sources[:3], 1):
            preview = source.page_content[:80] + "..." if len(source.page_content) > 80 else source.page_content
            explanation += f"{i}. {preview}\n"
        
        return explanation
    
    def _confidence_based_explanation(self, confidence: float) -> str:
        if confidence > 0.8:
            level = "üîµ –í–´–°–û–ö–ê–Ø"
        elif confidence > 0.5:
            level = "üü° –°–†–ï–î–ù–Ø–Ø"
        else:
            level = "üî¥ –ù–ò–ó–ö–ê–Ø"
        
        return f"üìä **–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å —Å–∏—Å—Ç–µ–º—ã:** {level} ({confidence:.1%})"
    
    def _process_based_explanation(self, agent_info: Dict) -> str:
        if agent_info["agent"] == "general":
            return "‚öôÔ∏è **–ü—Ä–æ—Ü–µ—Å—Å:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –æ–±—â–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º –∞–Ω–∞–ª–∏–∑–∞"
        
        agent_name = agent_info["agent"]
        confidence = agent_info["confidence"]
        
        return f"‚öôÔ∏è **–ü—Ä–æ—Ü–µ—Å—Å:** –í–æ–ø—Ä–æ—Å –æ–±—Ä–∞–±–æ—Ç–∞–Ω {agent_name}-–∞–≥–µ–Ω—Ç–æ–º (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})"

# ==================== 
# 5. REAL-TIME DATA ENRICHMENT
# ====================
class RealTimeDataEnricher:
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 300  # 5 –º–∏–Ω—É—Ç
        
    def enrich_response(self, response: str, question: str, context: List) -> str:
        """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        enriched_response = response
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –∫—É—Ä—Å–∞–º–∏ –≤–∞–ª—é—Ç
        if any(currency in question.lower() for currency in ['–∫—É—Ä—Å', '–µ–≤—Ä–æ', '–¥–æ–ª–ª–∞—Ä', '—Ä—É–±–ª']):
            rates = self._get_exchange_rates()
            enriched_response += f"\n\nüí± **–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ –∫—É—Ä—Å—ã:** {rates}"
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        if any(time_word in question.lower() for time_word in ['—Å–µ–π—á–∞—Å', '–∞–∫—Ç—É–∞–ª—å–Ω', '—Ç–µ–∫—É—â']):
            timestamp = datetime.now().strftime("%d.%m.%Y %H:%M")
            enriched_response += f"\n\nüïí **–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞ –Ω–∞:** {timestamp}"
        
        # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ä–∞—Å—á–µ—Ç–∞–º–∏
        if '—Ä–∞—Å—á–µ—Ç' in question.lower() or '—Ä–∞—Å—Å—á–∏—Ç–∞–π' in question.lower():
            calculation = self._provide_calculation(question, context)
            if calculation:
                enriched_response += f"\n\nüßÆ **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π —Ä–∞—Å—á–µ—Ç:** {calculation}"
        
        return enriched_response
    
    def _get_exchange_rates(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—É—Ä—Å–æ–≤ –≤–∞–ª—é—Ç (–∑–∞–≥–ª—É—à–∫–∞)"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ - API –¶–ë –†–§ –∏–ª–∏ –¥—Ä—É–≥–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        return "USD: 90.5‚ÇΩ | EUR: 98.2‚ÇΩ | CNY: 12.4‚ÇΩ"
    
    def _provide_calculation(self, question: str, context: List) -> str:
        """–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–∞—Å—á–µ—Ç–æ–≤"""
        question_lower = question.lower()
        
        if '–∫—Ä–µ–¥–∏—Ç' in question_lower:
            return "üí° –†–∞—Å—á–µ—Ç –µ–∂–µ–º–µ—Å—è—á–Ω–æ–≥–æ –ø–ª–∞—Ç–µ–∂–∞: —Å—É–º–º–∞ / —Å—Ä–æ–∫ * (1 + —Å—Ç–∞–≤–∫–∞/12)"
        elif '–≤–∫–ª–∞–¥' in question_lower:
            return "üí° –î–æ—Ö–æ–¥ –ø–æ –≤–∫–ª–∞–¥—É: —Å—É–º–º–∞ * —Å—Ç–∞–≤–∫–∞ * —Å—Ä–æ–∫ / 365"
        
        return ""

# ==================== 
# 6. A/B TESTING FRAMEWORK
# ====================
class ABTestingFramework:
    def __init__(self):
        self.models = {}
        self.test_results = {}
        self.active_tests = {}
    
    def register_model(self, name: str, model_config: Dict):
        """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.models[name] = model_config
    
    def run_comparison(self, question: str, context: List) -> Dict:
        """–ó–∞–ø—É—Å–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ–¥–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ"""
        results = {}
        
        for model_name, model_config in self.models.items():
            start_time = time.time()
            
            try:
                # –ó–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                response = f"–û—Ç–≤–µ—Ç –æ—Ç {model_name}: —Ç–µ—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç"
                end_time = time.time()
                
                results[model_name] = {
                    "response": response,
                    "response_time": end_time - start_time,
                    "quality_score": self._evaluate_response(question, response, context),
                    "timestamp": datetime.now().isoformat()
                }
            except Exception as e:
                results[model_name] = {
                    "error": str(e),
                    "response_time": 0,
                    "quality_score": 0
                }
        
        return results
    
    def _evaluate_response(self, question: str, response: str, context: List) -> float:
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
        score = 0.0
        
        # –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        if any(word in response.lower() for word in question.lower().split()[:3]):
            score += 0.3
        
        # –û—Ü–µ–Ω–∫–∞ –¥–ª–∏–Ω—ã
        if 20 < len(response) < 500:
            score += 0.4
        
        # –û—Ü–µ–Ω–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        if any(marker in response for marker in [':', '- ', '\n']):
            score += 0.3
            
        return score

# ==================== 
# 7. ADVANCED SECURITY MONITORING
# ====================
class AdvancedSecurityMonitor:
    def __init__(self):
        self.threat_patterns = {
            'data_scraping': [r'\b–≤—Å–µ\b.*\b–ø—Ä–æ–¥—É–∫—Ç', r'\b–ø–æ–ª–Ω—ã–π\b.*\b—Å–ø–∏—Å–æ–∫', r'\b—Å–∫–∞—á–∞–π\b.*\b–±–∞–∑'],
            'prompt_leaking': [r'\b–ø–æ–∫–∞–∂–∏\b.*\b–ø—Ä–æ–º–ø—Ç', r'\b—Å–∏—Å—Ç–µ–º–Ω\b.*\b—Å–æ–æ–±—â–µ–Ω', r'\b–∏—Å—Ö–æ–¥–Ω\b.*\b–∫–æ–¥'],
            'model_theft': [r'\b—Å–∫–∞—á–∞–π\b.*\b–º–æ–¥–µ–ª—å', r'\b–≤–µ—Å\b.*\b—Ñ–∞–π–ª', r'\b–ø–∞—Ä–∞–º–µ—Ç—Ä\b.*\b–Ω–µ–π—Ä–æ—Å–µ—Ç']
        }
        self.suspicious_activity = []
    
    def analyze_question_patterns(self, question: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ —É–≥—Ä–æ–∑—ã"""
        threats_detected = []
        confidence_scores = {}
        
        for threat_type, patterns in self.threat_patterns.items():
            for pattern in patterns:
                if re.search(pattern, question.lower()):
                    threats_detected.append(threat_type)
                    confidence_scores[threat_type] = confidence_scores.get(threat_type, 0) + 0.3
        
        return {
            "threats": threats_detected,
            "confidence": confidence_scores,
            "risk_level": self._calculate_risk_level(threats_detected, confidence_scores)
        }
    
    def _calculate_risk_level(self, threats: List, confidence: Dict) -> str:
        """–†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞"""
        if not threats:
            return "low"
        
        max_confidence = max(confidence.values()) if confidence else 0
        
        if max_confidence > 0.7:
            return "critical"
        elif max_confidence > 0.4:
            return "high"
        else:
            return "medium"
    
    def generate_security_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"""
        return f"""
üîí –û–¢–ß–ï–¢ –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò
‚Ä¢ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π: {len(self.suspicious_activity)}
‚Ä¢ –£—Ä–æ–≤–µ–Ω—å —É–≥—Ä–æ–∑: {self._get_overall_threat_level()}
‚Ä¢ –ü–æ—Å–ª–µ–¥–Ω—è—è –ø—Ä–æ–≤–µ—Ä–∫–∞: {datetime.now().strftime('%H:%M:%S')}
"""

# ==================== 
# 8. –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –í–°–ï–• –ú–û–î–£–õ–ï–ô
# ====================
class AdvancedRAGSystem:
    def __init__(self, base_rag_system):
        self.base_rag = base_rag_system
        self.agents = SpecializedAgents(base_rag_system)
        self.metrics = AdvancedMetrics()
        self.prompt_engine = AdaptivePromptEngine()
        self.xai = ExplainableAI()
        self.data_enricher = RealTimeDataEnricher()
        self.ab_testing = ABTestingFramework()
        self.security_monitor = AdvancedSecurityMonitor()
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self._setup_ab_testing()
    
    def _setup_ab_testing(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.ab_testing.register_model("qwen-fast", {"speed": "high", "quality": "medium"})
        self.ab_testing.register_model("qwen-quality", {"speed": "medium", "quality": "high"})
    
    def ask(self, question: str) -> Dict:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∑–∞–ø—Ä–æ—Å –∫ —Å–∏—Å—Ç–µ–º–µ"""
        start_time = time.time()
        
        # –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        security_analysis = self.security_monitor.analyze_question_patterns(question)
        
        if security_analysis["risk_level"] in ["high", "critical"]:
            return {
                "result": "‚ùå –ó–∞–ø—Ä–æ—Å –æ—Ç–∫–ª–æ–Ω–µ–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
                "security_risk": security_analysis
            }
        
        # –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∫ –∞–≥–µ–Ω—Ç—É
        agent_info = self.agents.route_question(question)
        self.metrics.log_agent_usage(agent_info["agent"])
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –±–∞–∑–æ–≤–æ–π RAG
            rag_result = self.base_rag.cached_qa(question)
            answer = rag_result['result']
            
            # –û–±–æ–≥–∞—â–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
            enriched_answer = self.data_enricher.enrich_response(
                answer, question, rag_result.get('source_documents', [])
            )
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
            explanation = self.xai.generate_explanation(
                question, answer,
                rag_result.get('source_documents', []),
                agent_info["confidence"],
                agent_info
            )
            
            # –†–∞—Å—á–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞
            quality_score = self.metrics.calculate_quality_score(
                question, answer, rag_result.get('source_documents', [])
            )
            self.metrics.quality_scores.append(quality_score)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            self.prompt_engine.update_conversation_history(question, answer)
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
            self.metrics.log_response_time(start_time)
            
            return {
                "result": enriched_answer,
                "explanation": explanation,
                "agent_used": agent_info["agent"],
                "confidence": agent_info["confidence"],
                "quality_score": quality_score,
                "response_time": time.time() - start_time,
                "security_analysis": security_analysis
            }
            
        except Exception as e:
            return {
                "result": f"‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}",
                "error": True
            }
    
    def get_system_report(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å–∏—Å—Ç–µ–º—ã"""
        quality_report = self.metrics.generate_quality_report()
        security_report = self.security_monitor.generate_security_report()
        
        return f"""
üè¶ –û–¢–ß–ï–¢ –ü–†–û–î–í–ò–ù–£–¢–û–ô RAG –°–ò–°–¢–ï–ú–´
{'='*60}
{quality_report}
{security_report}
{'='*60}
"""

# ==================== 
# 9. –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø
# ====================
def demonstrate_advanced_features():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"""
    
    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—É—é RAG —Å–∏—Å—Ç–µ–º—É (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
    class MockBaseRAG:
        def __init__(self):
            self.audit = type('Audit', (), {'request_count': 0})()
        
        @lru_cache(maxsize=50)
        def cached_qa(self, question):
            return {
                'result': f"–û—Ç–≤–µ—Ç –Ω–∞: {question}",
                'source_documents': []
            }
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã
    base_rag = MockBaseRAG()
    advanced_system = AdvancedRAGSystem(base_rag)
    
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–û–î–í–ò–ù–£–¢–´–• –í–û–ó–ú–û–ñ–ù–û–°–¢–ï–ô")
    print("=" * 60)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã
    test_questions = [
        "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω—É–∂–Ω—ã –¥–ª—è –∏–ø–æ—Ç–µ–∫–∏?",
        "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –∫—ç—à–±—ç–∫ –Ω–∞ –∫–∞—Ä—Ç–∞—Ö",
        "–ö–∞–∫–æ–π –¥–æ—Ö–æ–¥ –±—É–¥–µ—Ç —Å –≤–∫–ª–∞–¥–∞ 100000 —Ä—É–±–ª–µ–π?",
        "–ü–æ–∫–∞–∂–∏ –≤—Å–µ –∫—Ä–µ–¥–∏—Ç–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã"  # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n{i}. ‚ùì –í–æ–ø—Ä–æ—Å: {question}")
        
        result = advanced_system.ask(question)
        
        if "security_risk" in result and result["security_risk"]["risk_level"] != "low":
            print(f"   üîí –°—Ä–∞–±–æ—Ç–∞–ª–∞ –∑–∞—â–∏—Ç–∞: {result['result']}")
        else:
            print(f"   ‚úÖ –û—Ç–≤–µ—Ç: {result['result']}")
            print(f"   ü§ñ –ê–≥–µ–Ω—Ç: {result.get('agent_used', 'general')}")
            print(f"   üìä –ö–∞—á–µ—Å—Ç–≤–æ: {result.get('quality_score', 0):.1%}")
        
        print("   " + "-" * 40)
    
    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 60)
    print(advanced_system.get_system_report())

# –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
if __name__ == "__main__":
    demonstrate_advanced_features()